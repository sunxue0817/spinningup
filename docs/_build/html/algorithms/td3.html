

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Twin Delayed DDPG &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Soft Actor-Critic" href="sac.html" />
    <link rel="prev" title="Deep Deterministic Policy Gradient" href="ddpg.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">用户文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">运行试验</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">试验输出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">绘制结果</a></li>
</ul>
<p class="caption"><span class="caption-text">强化学习介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro.html">第一部分：强化学习中的核心概念</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro2.html">第二部分：各种各种的强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro3.html">第三部分：策略优化介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">资源</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/spinningup.html">Spinning Up as a Deep RL Researcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/keypapers.html">深度强化学习的核心论文</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/exercises.html">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/bench.html">Benchmarks for Spinning Up Implementations</a></li>
</ul>
<p class="caption"><span class="caption-text">算法文档</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Twin Delayed DDPG</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quick-facts">Quick Facts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#key-equations">Key Equations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exploration-vs-exploitation">Exploration vs. Exploitation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pseudocode">Pseudocode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#documentation">Documentation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#saved-model-contents">Saved Model Contents</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#relevant-papers">Relevant Papers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-public-implementations">Other Public Implementations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">工具文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">日志打印</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">绘图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI 工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">运行工具</a></li>
</ul>
<p class="caption"><span class="caption-text">其他</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">作者</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/translator.html">关于译者</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Twin Delayed DDPG</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/algorithms/td3.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="twin-delayed-ddpg">
<h1><a class="toc-backref" href="#id1">Twin Delayed DDPG</a><a class="headerlink" href="#twin-delayed-ddpg" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#twin-delayed-ddpg" id="id1">Twin Delayed DDPG</a><ul>
<li><a class="reference internal" href="#background" id="id2">Background</a><ul>
<li><a class="reference internal" href="#quick-facts" id="id3">Quick Facts</a></li>
<li><a class="reference internal" href="#key-equations" id="id4">Key Equations</a></li>
<li><a class="reference internal" href="#exploration-vs-exploitation" id="id5">Exploration vs. Exploitation</a></li>
<li><a class="reference internal" href="#pseudocode" id="id6">Pseudocode</a></li>
</ul>
</li>
<li><a class="reference internal" href="#documentation" id="id7">Documentation</a><ul>
<li><a class="reference internal" href="#saved-model-contents" id="id8">Saved Model Contents</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references" id="id9">References</a><ul>
<li><a class="reference internal" href="#relevant-papers" id="id10">Relevant Papers</a></li>
<li><a class="reference internal" href="#other-public-implementations" id="id11">Other Public Implementations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="background">
<h2><a class="toc-backref" href="#id2">Background</a><a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>(Previously: <a class="reference external" href="../algorithms/ddpg.html#background">Background for DDPG</a>)</p>
<p>While DDPG can achieve great performance sometimes, it is frequently brittle with respect to hyperparameters and other kinds of tuning. A common failure mode for DDPG is that the learned Q-function begins to dramatically overestimate Q-values, which then leads to the policy breaking, because it exploits the errors in the Q-function. Twin Delayed DDPG (TD3) is an algorithm which addresses this issue by introducing three critical tricks:</p>
<p><strong>Trick One: Clipped Double-Q Learning.</strong> TD3 learns <em>two</em> Q-functions instead of one (hence &#8220;twin&#8221;), and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.</p>
<p><strong>Trick Two: &#8220;Delayed&#8221; Policy Updates.</strong> TD3 updates the policy (and target networks) less frequently than the Q-function. The paper recommends one policy update for every two Q-function updates.</p>
<p><strong>Trick Three: Target Policy Smoothing.</strong> TD3 adds noise to the target action, to make it harder for the policy to exploit Q-function errors by smoothing out Q along changes in action.</p>
<p>Together, these three tricks result in substantially improved performance over baseline DDPG.</p>
<div class="section" id="quick-facts">
<h3><a class="toc-backref" href="#id3">Quick Facts</a><a class="headerlink" href="#quick-facts" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>TD3 is an off-policy algorithm.</li>
<li>TD3 can only be used for environments with continuous action spaces.</li>
<li>The Spinning Up implementation of TD3 does not support parallelization.</li>
</ul>
</div>
<div class="section" id="key-equations">
<h3><a class="toc-backref" href="#id4">Key Equations</a><a class="headerlink" href="#key-equations" title="Permalink to this headline">¶</a></h3>
<p>TD3 concurrently learns two Q-functions, <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">Q_{\phi_1}</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
 and <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">Q_{\phi_2}</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
, by mean square Bellman error minimization, in almost the same way that DDPG learns its single Q-function. To show exactly how TD3 does this and how it differs from normal DDPG, we&#8217;ll work from the innermost part of the loss function outwards.</p>
<p>First: <strong>target policy smoothing</strong>. Actions used to form the Q-learning target are based on the target policy, <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">\mu_{\theta_{\text{targ}}}</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
, but with clipped noise added on each dimension of the action. After adding the clipped noise, the target action is then clipped to lie in the valid action range (all valid actions, <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">a</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
, satisfy <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">a_{Low} \leq a \leq a_{High}</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
). The target actions are thus:</p>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">a'(s') = \text{clip}\left(\mu_{\theta_{\text{targ}}}(s') + \text{clip}(\epsilon,-c,c), a_{Low}, a_{High}\right), \;\;\;\;\; \epsilon \sim \mathcal{N}(0, \sigma)</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
<p>Target policy smoothing essentially serves as a regularizer for the algorithm. It addresses a particular failure mode that can happen in DDPG: if the Q-function approximator develops an incorrect sharp peak for some actions, the policy will quickly exploit that peak and then have brittle or incorrect behavior. This can be averted by smoothing out the Q-function over similar actions, which target policy smoothing is designed to do.</p>
<p>Next: <strong>clipped double-Q learning</strong>. Both Q-functions use a single target, calculated using whichever of the two Q-functions gives a smaller target value:</p>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">y(r,s',d) = r + \gamma (1 - d) \min_{i=1,2} Q_{\phi_{i, \text{targ}}}(s', a'(s')),</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
<p>and then both are learned by regressing to this target:</p>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">L(\phi_1, {\mathcal D}) = \underE{(s,a,r,s',d) \sim {\mathcal D}}{
    \Bigg( Q_{\phi_1}(s,a) - y(r,s',d) \Bigg)^2
    },</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">L(\phi_2, {\mathcal D}) = \underE{(s,a,r,s',d) \sim {\mathcal D}}{
    \Bigg( Q_{\phi_2}(s,a) - y(r,s',d) \Bigg)^2
    }.</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
<p>Using the smaller Q-value for the target, and regressing towards that, helps fend off overestimation in the Q-function.</p>
<p>Lastly: the policy is learned just by maximizing <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">Q_{\phi_1}</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
:</p>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">\max_{\theta} \underset{s \sim {\mathcal D}}{{\mathrm E}}\left[ Q_{\phi_1}(s, \mu_{\theta}(s)) \right],</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
<p>which is pretty much unchanged from DDPG. However, in TD3, the policy is updated less frequently than the Q-functions are. This helps damp the volatility that normally arises in DDPG because of how a policy update changes the target.</p>
</div>
<div class="section" id="exploration-vs-exploitation">
<h3><a class="toc-backref" href="#id5">Exploration vs. Exploitation</a><a class="headerlink" href="#exploration-vs-exploitation" title="Permalink to this headline">¶</a></h3>
<p>TD3 trains a deterministic policy in an off-policy way. Because the policy is deterministic, if the agent were to explore on-policy, in the beginning it would probably not try a wide enough variety of actions to find useful learning signals. To make TD3 policies explore better, we add noise to their actions at training time, typically uncorrelated mean-zero Gaussian noise. To facilitate getting higher-quality training data, you may reduce the scale of the noise over the course of training. (We do not do this in our implementation, and keep noise scale fixed throughout.)</p>
<p>At test time, to see how well the policy exploits what it has learned, we do not add noise to the actions.</p>
<div class="admonition-you-should-know admonition">
<p class="first admonition-title">You Should Know</p>
<p class="last">Our TD3 implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning (set with the <code class="docutils literal"><span class="pre">start_steps</span></code> keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal TD3 exploration.</p>
</div>
</div>
<div class="section" id="pseudocode">
<h3><a class="toc-backref" href="#id6">Pseudocode</a><a class="headerlink" href="#pseudocode" title="Permalink to this headline">¶</a></h3>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">\begin{algorithm}[H]
    \caption{Twin Delayed DDPG}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta$, Q-function parameters $\phi_1$, $\phi_2$, empty replay buffer $\mathcal{D}$
    \STATE Set target parameters equal to main parameters $\theta_{\text{targ}} \leftarrow \theta$, $\phi_{\text{targ},1} \leftarrow \phi_1$, $\phi_{\text{targ},2} \leftarrow \phi_2$
    \REPEAT
        \STATE Observe state $s$ and select action $a = \text{clip}(\mu_{\theta}(s) + \epsilon, a_{Low}, a_{High})$, where $\epsilon \sim \mathcal{N}$
        \STATE Execute $a$ in the environment
        \STATE Observe next state $s'$, reward $r$, and done signal $d$ to indicate whether $s'$ is terminal
        \STATE Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$
        \STATE If $s'$ is terminal, reset environment state.
        \IF{it's time to update}
            \FOR{$j$ in range(however many updates)}
                \STATE Randomly sample a batch of transitions, $B = \{ (s,a,r,s',d) \}$ from $\mathcal{D}$
                \STATE Compute target actions
                \begin{equation*}
                    a'(s') = \text{clip}\left(\mu_{\theta_{\text{targ}}}(s') + \text{clip}(\epsilon,-c,c), a_{Low}, a_{High}\right), \;\;\;\;\; \epsilon \sim \mathcal{N}(0, \sigma)
                \end{equation*}
                \STATE Compute targets
                \begin{equation*}
                    y(r,s',d) = r + \gamma (1-d) \min_{i=1,2} Q_{\phi_{\text{targ},i}}(s', a'(s'))
                \end{equation*}
                \STATE Update Q-functions by one step of gradient descent using
                \begin{align*}
                    &amp; \nabla_{\phi_i} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi,i}(s,a) - y(r,s',d) \right)^2 &amp;&amp; \text{for } i=1,2
                \end{align*}
                \IF{ $j \mod$ \texttt{policy\_delay} $ = 0$}
                    \STATE Update policy by one step of gradient ascent using
                    \begin{equation*}
                        \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B}Q_{\phi,1}(s, \mu_{\theta}(s))
                    \end{equation*}
                    \STATE Update target networks with
                    \begin{align*}
                        \phi_{\text{targ},i} &amp;\leftarrow \rho \phi_{\text{targ},i} + (1-\rho) \phi_i &amp;&amp; \text{for } i=1,2\\
                        \theta_{\text{targ}} &amp;\leftarrow \rho \theta_{\text{targ}} + (1-\rho) \theta
                    \end{align*}
                \ENDIF
            \ENDFOR
        \ENDIF
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
</div>
</div>
<div class="section" id="documentation">
<h2><a class="toc-backref" href="#id7">Documentation</a><a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="spinup.td3">
<code class="descclassname">spinup.</code><code class="descname">td3</code><span class="sig-paren">(</span><em>env_fn</em>, <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>, <em>ac_kwargs={}</em>, <em>seed=0</em>, <em>steps_per_epoch=5000</em>, <em>epochs=100</em>, <em>replay_size=1000000</em>, <em>gamma=0.99</em>, <em>polyak=0.995</em>, <em>pi_lr=0.001</em>, <em>q_lr=0.001</em>, <em>batch_size=100</em>, <em>start_steps=10000</em>, <em>act_noise=0.1</em>, <em>target_noise=0.2</em>, <em>noise_clip=0.5</em>, <em>policy_delay=2</em>, <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>, <em>save_freq=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/spinup/algos/td3/td3.html#td3"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#spinup.td3" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_fn</strong> &#8211; A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</li>
<li><strong>actor_critic</strong> &#8211; <p>A function which takes in placeholder symbols
for state, <code class="docutils literal"><span class="pre">x_ph</span></code>, and action, <code class="docutils literal"><span class="pre">a_ph</span></code>, and returns the main
outputs from the agent&#8217;s Tensorflow computation graph:</p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="25%" />
<col width="58%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Symbol</th>
<th class="head">Shape</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Deterministically computes actions</div>
<div class="line">from policy given states.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">q1</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives one estimate of Q* for</div>
<div class="line">states in <code class="docutils literal"><span class="pre">x_ph</span></code> and actions in</div>
<div class="line"><code class="docutils literal"><span class="pre">a_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">q2</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives another estimate of Q* for</div>
<div class="line">states in <code class="docutils literal"><span class="pre">x_ph</span></code> and actions in</div>
<div class="line"><code class="docutils literal"><span class="pre">a_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">q1_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the composition of <code class="docutils literal"><span class="pre">q1</span></code> and</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code> for states in <code class="docutils literal"><span class="pre">x_ph</span></code>:</div>
<div class="line">q1(x, pi(x)).</div>
</div>
</td>
</tr>
</tbody>
</table>
</li>
<li><strong>ac_kwargs</strong> (<em>dict</em>) &#8211; Any kwargs appropriate for the actor_critic
function you provided to TD3.</li>
<li><strong>seed</strong> (<em>int</em>) &#8211; Seed for random number generators.</li>
<li><strong>steps_per_epoch</strong> (<em>int</em>) &#8211; Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</li>
<li><strong>epochs</strong> (<em>int</em>) &#8211; Number of epochs to run and train agent.</li>
<li><strong>replay_size</strong> (<em>int</em>) &#8211; Maximum length of replay buffer.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; Discount factor. (Always between 0 and 1.)</li>
<li><strong>polyak</strong> (<em>float</em>) &#8211; <p>Interpolation factor in polyak averaging for target
networks. Target networks are updated towards main networks
according to:</p>
<div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta

</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
<p>where <div class="system-message">
<p class="system-message-title">System Message: WARNING/2 (<tt class="docutils">\rho</tt>)</p>
latex exited with error
[stdout]
This is pdfTeX, Version 3.14159265-2.6-1.40.19 (TeX Live 2018) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode
(./math.tex
LaTeX2e &lt;2018-04-01&gt; patch level 2
Babel &lt;3.18&gt; and hyphenation patterns for 22 language(s) loaded.
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/size12.clo))
(/usr/local/texlive/2018basic/texmf-dist/tex/latex/base/inputenc.sty

! LaTeX Error: File `utf8x.def&#8217; not found.

Type X to quit or &lt;RETURN&gt; to proceed,
or enter new name. (Default extension: def)

Enter file name: 
! Emergency stop.
&lt;read *&gt; 
         
l.165 \endinput
               ^^M
No pages of output.
Transcript written on math.log.
</div>
 is polyak. (Always between 0 and 1, usually
close to 1.)</p>
</li>
<li><strong>pi_lr</strong> (<em>float</em>) &#8211; Learning rate for policy.</li>
<li><strong>q_lr</strong> (<em>float</em>) &#8211; Learning rate for Q-networks.</li>
<li><strong>batch_size</strong> (<em>int</em>) &#8211; Minibatch size for SGD.</li>
<li><strong>start_steps</strong> (<em>int</em>) &#8211; Number of steps for uniform-random action selection,
before running real policy. Helps exploration.</li>
<li><strong>act_noise</strong> (<em>float</em>) &#8211; Stddev for Gaussian exploration noise added to
policy at training time. (At test time, no noise is added.)</li>
<li><strong>target_noise</strong> (<em>float</em>) &#8211; Stddev for smoothing noise added to target
policy.</li>
<li><strong>noise_clip</strong> (<em>float</em>) &#8211; Limit for absolute value of target policy
smoothing noise.</li>
<li><strong>policy_delay</strong> (<em>int</em>) &#8211; Policy will only be updated once every
policy_delay times for each update of the Q-networks.</li>
<li><strong>max_ep_len</strong> (<em>int</em>) &#8211; Maximum length of trajectory / episode / rollout.</li>
<li><strong>logger_kwargs</strong> (<em>dict</em>) &#8211; Keyword args for EpochLogger.</li>
<li><strong>save_freq</strong> (<em>int</em>) &#8211; How often (in terms of gap between epochs) to save
the current policy and value function.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="section" id="saved-model-contents">
<h3><a class="toc-backref" href="#id8">Saved Model Contents</a><a class="headerlink" href="#saved-model-contents" title="Permalink to this headline">¶</a></h3>
<p>The computation graph saved by the logger includes:</p>
<table border="1" class="docutils">
<colgroup>
<col width="9%" />
<col width="91%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">x</span></code></td>
<td>Tensorflow placeholder for state input.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">a</span></code></td>
<td>Tensorflow placeholder for action input.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td><div class="first last line-block">
<div class="line">Deterministically computes an action from the agent, conditioned</div>
<div class="line">on states in <code class="docutils literal"><span class="pre">x</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">q1</span></code></td>
<td>Gives one action-value estimate for states in <code class="docutils literal"><span class="pre">x</span></code> and actions in <code class="docutils literal"><span class="pre">a</span></code>.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">q2</span></code></td>
<td>Gives the other action-value estimate for states in <code class="docutils literal"><span class="pre">x</span></code> and actions in <code class="docutils literal"><span class="pre">a</span></code>.</td>
</tr>
</tbody>
</table>
<p>This saved model can be accessed either by</p>
<ul class="simple">
<li>running the trained policy with the <a class="reference external" href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a> tool,</li>
<li>or loading the whole saved graph into a program with <a class="reference external" href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a>.</li>
</ul>
</div>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id9">References</a><a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<div class="section" id="relevant-papers">
<h3><a class="toc-backref" href="#id10">Relevant Papers</a><a class="headerlink" href="#relevant-papers" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1802.09477">Addressing Function Approximation Error in Actor-Critic Methods</a>, Fujimoto et al, 2018</li>
</ul>
</div>
<div class="section" id="other-public-implementations">
<h3><a class="toc-backref" href="#id11">Other Public Implementations</a><a class="headerlink" href="#other-public-implementations" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/sfujim/TD3">TD3 release repo</a></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="sac.html" class="btn btn-neutral float-right" title="Soft Actor-Critic" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ddpg.html" class="btn btn-neutral" title="Deep Deterministic Policy Gradient" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI .

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>